#!/usr/bin/env python3
"""
ç®€åŒ–çš„Leanè¯æ˜ä¿®å¤Pipelineæ¼”ç¤º
ä¸“æ³¨äºå±•ç¤ºåˆ†è§£solverå¯¹åŒ…å«haveåµŒå¥—ç»“æ„é—®é¢˜çš„å¤„ç†èƒ½åŠ›
"""

import logging
import json
import os
from datetime import datetime
from pathlib import Path

# å¯¼å…¥å¿…è¦çš„ç±»
from data_management.unified_problem_manager import Problem, UnifiedProblemManager
from solvers.decompose_solver_unified import DecomposeSolver

def setup_logging():
    """è®¾ç½®æ—¥å¿—"""
    # åˆ›å»ºæ—¥å¿—ç›®å½•
    log_dir = Path("demo_pipeline_logs")
    log_dir.mkdir(exist_ok=True)
    
    # è®¾ç½®æ—¥å¿—æ–‡ä»¶
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    log_file = log_dir / f"simple_pipeline_{timestamp}.log"
    
    # é…ç½®æ—¥å¿—
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(levelname)s - %(message)s',
        handlers=[
            logging.FileHandler(log_file, encoding='utf-8'),
            logging.StreamHandler()
        ]
    )
    
    return str(log_file)

def load_demo_problems():
    """åŠ è½½æœ‰æ„ä¹‰çš„demoé—®é¢˜ï¼ˆåŒ…å«haveåµŒå¥—ï¼‰"""
    with open('data/unified_problems_metadata.json', 'r') as f:
        all_metadata = json.load(f)
    
    # ç­›é€‰demoé—®é¢˜ï¼Œä¼˜å…ˆé€‰æ‹©å¤æ‚çš„haveç»“æ„
    demo_problems = {}
    priority_problems = ['nested_have', 'complex_have', 'hole_proof']
    
    for problem_id, metadata in all_metadata.items():
        if metadata.get('dataset') == 'demo':
            demo_problems[problem_id] = metadata
    
    # æŒ‰ä¼˜å…ˆçº§æ’åº
    sorted_problems = {}
    for priority in priority_problems:
        key = f"demo/{priority}"
        if key in demo_problems:
            sorted_problems[key] = demo_problems[key]
    
    # æ·»åŠ å…¶ä»–demoé—®é¢˜
    for key, metadata in demo_problems.items():
        if key not in sorted_problems:
            sorted_problems[key] = metadata
    
    return sorted_problems

def create_problem_from_metadata(metadata_info):
    """ä»metadataåˆ›å»ºProblemå®ä¾‹"""
    try:
        # è·¯å¾„æ˜ å°„ï¼šdemo -> demo
        dataset = metadata_info['dataset']
        if dataset == 'demo':
            dataset = 'demo'
        
        problem_id = metadata_info['problem_id']
        
        # æ„å»ºæ–‡ä»¶è·¯å¾„
        header_path = f"unified_problems/{dataset}/{problem_id}/header.lean"
        problem_path = f"unified_problems/{dataset}/{problem_id}/problem.lean"
        decomposed_dir = f"unified_problems/{dataset}/{problem_id}/decomposed"
        hole_dir = f"unified_problems/{dataset}/{problem_id}/hole"
        
        # è¯»å–æ–‡ä»¶
        with open(header_path, 'r') as f:
            header_content = f.read()
        with open(problem_path, 'r') as f:
            problem_content = f.read()
        
        # åˆ›å»ºProblemå®ä¾‹
        problem = Problem(
            dataset=metadata_info['dataset'],
            problem_id=problem_id,
            header_path=header_path,
            problem_path=problem_path,
            decomposed_dir=decomposed_dir,
            hole_dir=hole_dir,
            original_path=None
        )
        
        # æ·»åŠ å†…å®¹å±æ€§
        problem.content = header_content + "\n" + problem_content
        
        return problem
    except Exception as e:
        print(f"âŒ Error creating problem: {e}")
        return None

def count_have_statements(content):
    """ç»Ÿè®¡haveè¯­å¥æ•°é‡"""
    lines = content.split('\n')
    have_count = 0
    nested_level = 0
    max_nested = 0
    
    for line in lines:
        stripped = line.strip()
        if stripped.startswith('have '):
            have_count += 1
            # ç®€å•çš„åµŒå¥—çº§åˆ«è®¡ç®—
            indent = len(line) - len(line.lstrip())
            current_level = indent // 2
            max_nested = max(max_nested, current_level)
    
    return have_count, max_nested

def demo_simple_pipeline():
    """è¿è¡Œç®€åŒ–çš„pipelineæ¼”ç¤º"""
    print("ğŸš€ Simple Lean Proof Pipeline Demo (Focus on Decomposition)")
    print("=" * 70)
    
    # è®¾ç½®æ—¥å¿—
    log_file = setup_logging()
    logging.info("Simple pipeline demo started")
    
    # åŠ è½½demoé—®é¢˜
    demo_problems = load_demo_problems()
    print(f"ğŸ“‹ Found {len(demo_problems)} demo problems:")
    for problem_id in demo_problems:
        print(f"  - {problem_id}")
    
    # åˆ›å»ºåˆ†è§£solver
    print("\nğŸ”§ Initializing Decompose Solver...")
    solver = DecomposeSolver()
    print(f"âœ… Solver ready: {solver.get_solver_info()['name']}")
    
    # å¤„ç†æ¯ä¸ªé—®é¢˜
    successful_decompositions = 0
    problems_with_have = 0
    
    for problem_id, metadata_info in demo_problems.items():
        print(f"\nğŸ§ª Processing: {problem_id}")
        print("=" * 60)
        
        # åˆ›å»ºproblemå®ä¾‹
        problem = create_problem_from_metadata(metadata_info)
        if not problem:
            continue
        
        # åˆ†æproblemå†…å®¹
        have_count, max_nested = count_have_statements(problem.content)
        if have_count > 0:
            problems_with_have += 1
            print(f"ğŸ“Š Analysis: {have_count} have statements, max nesting level: {max_nested}")
        else:
            print("ğŸ“Š Analysis: No have statements found")
        
        print(f"ğŸ“ Original problem:")
        print(problem.content)
        print()
        
        # è¿è¡Œåˆ†è§£solver
        print("ğŸ”§ Running decomposition...")
        try:
            result = solver.solve(problem)
            
            print(f"ğŸ“Š Decomposition result:")
            print(f"  Status: {result.status}")
            print(f"  Is Success: {result.is_success()}")
            
            if result.is_success():
                successful_decompositions += 1
                print("âœ… Decomposition successful!")
                
                # æ˜¾ç¤ºç»“æœ
                if hasattr(result, 'solution') and result.solution:
                    print(f"ğŸ“ Decomposed content:")
                    print(result.solution)
                
                # æ£€æŸ¥åˆ†è§£åçš„æ–‡ä»¶
                decomposed_dir = Path(problem.decomposed_dir)
                if decomposed_dir.exists():
                    decomposed_files = list(decomposed_dir.glob("*.lean"))
                    if decomposed_files:
                        print(f"ğŸ“ Generated {len(decomposed_files)} decomposed files:")
                        for f in decomposed_files:
                            print(f"  - {f.name}")
                    else:
                        print("ğŸ“ No decomposed files generated")
                else:
                    print("ğŸ“ Decomposed directory not created")
            else:
                print("âŒ Decomposition failed")
                if hasattr(result, 'error_message'):
                    print(f"   Error: {result.error_message}")
        
        except Exception as e:
            print(f"âŒ Exception during decomposition: {e}")
            logging.exception(f"Error processing {problem_id}")
    
    # æ€»ç»“
    print(f"\nğŸ Pipeline Demo Completed!")
    print("=" * 70)
    print(f"ğŸ“Š Summary:")
    print(f"  Total problems processed: {len(demo_problems)}")
    print(f"  Problems with have statements: {problems_with_have}")
    print(f"  Successful decompositions: {successful_decompositions}")
    print(f"  Success rate: {successful_decompositions/len(demo_problems)*100:.1f}%")
    print(f"ğŸ“‹ Full log saved to: {log_file}")

if __name__ == "__main__":
    demo_simple_pipeline() 